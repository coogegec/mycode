{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20가지 주제의 뉴스 데이터\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset=fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents=dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 카테고리\n",
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-6db4ffd17331>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc']=news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # 알파벳 이외의 문자 제거\n"
     ]
    }
   ],
   "source": [
    "news_df=pd.DataFrame({'document': documents})\n",
    "news_df['clean_doc']=news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # 알파벳 이외의 문자 제거\n",
    "# 길이가 3 이하인 단어 제거\n",
    "news_df['clean_doc']=news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "news_df['clean_doc']=news_df['clean_doc'].apply(lambda x: x.lower()) # 소문자 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well sure about story seem biased what disagree with your statement that media ruin israels reputation that rediculous media most israeli media world having lived europe realize that incidences such described letter have occured media whole seem ignore them subsidizing israels existance europeans least same degree think that might reason they report more clearly atrocities what shame that austria daily reports inhuman acts commited israeli soldiers blessing received from government makes some holocaust guilt away after look jews treating other races when they power unfortunate'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "tokenized_doc=news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "tokenized_doc=tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) # 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'sure', 'story', 'seem', 'biased', 'disagree', 'statement', 'media', 'ruin', 'israels', 'reputation', 'rediculous', 'media', 'israeli', 'media', 'world', 'lived', 'europe', 'realize', 'incidences', 'described', 'letter', 'occured', 'media', 'whole', 'seem', 'ignore', 'subsidizing', 'israels', 'existance', 'europeans', 'least', 'degree', 'think', 'might', 'reason', 'report', 'clearly', 'atrocities', 'shame', 'austria', 'daily', 'reports', 'inhuman', 'acts', 'commited', 'israeli', 'soldiers', 'blessing', 'received', 'government', 'makes', 'holocaust', 'guilt', 'away', 'look', 'jews', 'treating', 'races', 'power', 'unfortunate']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well sure story seem biased disagree statement media ruin israels reputation rediculous media israeli media world lived europe realize incidences described letter occured media whole seem ignore subsidizing israels existance europeans least degree think might reason report clearly atrocities shame austria daily reports inhuman acts commited israeli soldiers blessing received government makes holocaust guilt away look jews treating races power unfortunate'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF 행렬을 만들기 위해 다시 역토큰화\n",
    "detokenized_doc=[]\n",
    "for i in range(len(news_df)):\n",
    "    t=' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc']=detokenized_doc\n",
    "news_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 상위 1000개의 단어만 처리\n",
    "vectorizer=TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X=vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape # TF-IDF 행렬의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# 행렬 특이값 분해, 11314개의 행을 20개로 축소, n_components : 토픽 수\n",
    "svd_model=TruncatedSVD(n_components=20)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 토픽수 × 단어수\n",
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01469448,  0.05019035,  0.02132607, ...,  0.07865954,\n",
       "         0.01432356,  0.01788786],\n",
       "       [-0.00535905,  0.01654529, -0.01646425, ..., -0.06366315,\n",
       "        -0.01063321, -0.01904601],\n",
       "       [ 0.0017784 , -0.00369187, -0.01793139, ...,  0.05927518,\n",
       "         0.02630429,  0.02235225],\n",
       "       ...,\n",
       "       [-0.00588063, -0.00051538,  0.00320784, ...,  0.05444411,\n",
       "        -0.01236747, -0.00061163],\n",
       "       [ 0.00478448, -0.00386492,  0.00247843, ..., -0.06105495,\n",
       "        -0.01706505, -0.00657359],\n",
       "       [-0.00069927,  0.03093023,  0.00785223, ..., -0.01384836,\n",
       "         0.00432729, -0.00058995]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 :  [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
      "Topic 2 :  [('thanks', 0.32886), ('windows', 0.29091), ('card', 0.18063), ('drive', 0.1745), ('mail', 0.1511)]\n",
      "Topic 3 :  [('game', 0.37064), ('team', 0.32451), ('year', 0.28249), ('games', 0.25267), ('season', 0.1839)]\n",
      "Topic 4 :  [('drive', 0.53403), ('scsi', 0.20037), ('hard', 0.15659), ('disk', 0.15613), ('card', 0.14047)]\n",
      "Topic 5 :  [('windows', 0.40477), ('file', 0.25042), ('window', 0.18187), ('files', 0.15971), ('program', 0.13984)]\n",
      "Topic 6 :  [('chip', 0.1612), ('government', 0.16012), ('space', 0.15681), ('mail', 0.15669), ('information', 0.13383)]\n",
      "Topic 7 :  [('like', 0.66992), ('bike', 0.13975), ('know', 0.11676), ('chip', 0.11286), ('sounds', 0.10372)]\n",
      "Topic 8 :  [('card', 0.47476), ('video', 0.22503), ('sale', 0.2136), ('monitor', 0.16581), ('offer', 0.1486)]\n",
      "Topic 9 :  [('know', 0.46102), ('card', 0.33279), ('chip', 0.17094), ('government', 0.15079), ('video', 0.14198)]\n",
      "Topic 10 :  [('good', 0.44195), ('know', 0.21932), ('time', 0.16096), ('bike', 0.09682), ('jesus', 0.09118)]\n",
      "Topic 11 :  [('think', 0.77482), ('chip', 0.1099), ('need', 0.10714), ('thanks', 0.09579), ('people', 0.09128)]\n",
      "Topic 12 :  [('thanks', 0.38849), ('good', 0.21475), ('right', 0.21106), ('problem', 0.20758), ('bike', 0.19069)]\n",
      "Topic 13 :  [('good', 0.38271), ('people', 0.34516), ('windows', 0.29063), ('know', 0.2131), ('file', 0.20709)]\n",
      "Topic 14 :  [('space', 0.35143), ('think', 0.21233), ('know', 0.18218), ('nasa', 0.13715), ('israel', 0.11386)]\n",
      "Topic 15 :  [('israel', 0.3739), ('israeli', 0.19409), ('right', 0.18918), ('think', 0.16764), ('sale', 0.15221)]\n",
      "Topic 16 :  [('good', 0.31812), ('israel', 0.28004), ('card', 0.24323), ('space', 0.20522), ('file', 0.16337)]\n",
      "Topic 17 :  [('problem', 0.41505), ('good', 0.30437), ('people', 0.19881), ('israel', 0.16148), ('file', 0.13206)]\n",
      "Topic 18 :  [('time', 0.55113), ('make', 0.1744), ('windows', 0.16558), ('file', 0.15517), ('sale', 0.11768)]\n",
      "Topic 19 :  [('file', 0.26734), ('game', 0.21708), ('looking', 0.19642), ('right', 0.13837), ('work', 0.12809)]\n",
      "Topic 20 :  [('time', 0.40292), ('window', 0.20576), ('mail', 0.17833), ('right', 0.13657), ('read', 0.11842)]\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합, 1000개의 단어\n",
    "terms=vectorizer.get_feature_names()\n",
    "\n",
    "# 20개의 뉴스 그룹별로 추출한 토픽 리스트 출력\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d : \" % (idx+1),\n",
    "        [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "get_topics(svd_model.components_, terms)\n",
    "# 각 토픽의 핵심 키워드 추출\n",
    "# LSA : 쉽고 빠르게 구현이 가능하지만 새로운 데이터가 추가되면 처음부터 다시 계산을 해야 하는 단점이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.84432202,  0.20850925,  0.46823094],\n",
       "       [ 4.09935419, -0.13332384, -0.49268129],\n",
       "       [-1.70050345, -3.04924012,  0.76869479],\n",
       "       [-2.21379239, -3.06986255, -0.66041977],\n",
       "       [-2.102115  ,  3.36337224,  0.55345806]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "# 테스트할 데이터\n",
    "X=np.array([\n",
    "    [1,1,1,1,1,1,1,1,1,1,3,3,3,3,3,4,5,6],\n",
    "    [1,2,1,1,1,1,1,1,1,1,3,4,3,3,3,4,5,6],\n",
    "    [3,3,3,3,3,1,1,1,1,1,1,1,1,1,1,5,4,6],\n",
    "    [3,4,3,3,3,1,2,1,1,1,1,1,1,1,1,5,4,5],\n",
    "    [1,1,1,1,1,3,3,3,3,3,1,1,1,1,1,6,4,5],\n",
    "    [1,2,1,1,1,3,3,3,2,3,1,1,1,1,1,5,4,5],\n",
    "])\n",
    "# 주성분 분석(3개의 주성분으로 축소)\n",
    "pca=PCA(n_components=3)\n",
    "X2D=pca.fit_transform(X)\n",
    "X2D[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54\n",
      "0.42\n",
      "0.02\n"
     ]
    }
   ],
   "source": [
    "# 주성분 1은 54%, 주성분 2는 42%, 주성분 3은 2%, 나머지가 2% (주성분 1,2만으로 96%를 설명함)\n",
    "# 각 주성분의 축에 해당하는 데이터셋의 분산 비율\n",
    "for i in pca.explained_variance_ratio_:\n",
    "    print(f'{i:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.01080967, 1.1237144 , 1.01080967, 1.01080967, 1.01080967,\n",
       "       1.04652366, 0.87642192, 1.04652366, 1.15635772, 1.04652366,\n",
       "       2.94266667, 3.3355072 , 2.94266667, 2.94266667, 2.94266667,\n",
       "       4.16176255, 4.97133333, 6.14683992])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3개의 주성분으로 축소된 자료를 원래의 형태로 복원(정보의 손실이 있음)\n",
    "X3D_inv=pca.inverse_transform(X2D)\n",
    "X3D_inv[0]\n",
    "# 복원 단계에서 정보 손실이 발생할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015189685531841413"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 재구성 오차(reconstruction error) 계산 : 원본 데이터와 압축 후 원복한 데이터 사이의 평균 제곱 거리\n",
    "1 - pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 압축\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist=fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=mnist['data']\n",
    "y=mnist['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 적절한 차원의 수 선택\n",
    "# 분산을 95%로 유지하는 차원의 수 계산\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca=PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum=np.cumsum(pca.explained_variance_ratio_)\n",
    "d=np.argmax(cumsum >= 0.95) + 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "0.9503238973449987\n"
     ]
    }
   ],
   "source": [
    "# 분산 비율을 직접 지정하는 방식\n",
    "pca=PCA(n_components=0.95)\n",
    "X_reduced=pca.fit_transform(X_train)\n",
    "print(pca.n_components_)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 154 차원으로 압축\n",
    "pca=PCA(n_components=154)\n",
    "X_reduced=pca.fit_transform(X_train)\n",
    "# 784 차원으로 복원\n",
    "X_recovered=pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "# 대량의 데이터의 경우 pca를 구현하기 위해 전체 데이터셋을 메모리에 올리는 것이 어려울 수 있음\n",
    "# 점진적 pca(Incremental PCA) 알고리즘을 사용하여 미니배치 방법으로 pca를 실행할 수 있음\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches=100\n",
    "inc_pca=IncrementalPCA(n_components=154)\n",
    "# 미니배치에 해당하는 부분만 사용하므로 메모리가 절약됨\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    print(\".\", end=\"\")\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced=inc_pca.transform(X_train)\n",
    "X_recovered_inc_pca=inc_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일반 PCA와 점진적 PCA로 MNIST 데이터를 변환한 결과 비교\n",
    "# 평균이 같은지 확인\n",
    "# allclose() : 두 배열이 오차범위 내에서 같으면 True, 다르면 False\n",
    "np.allclose(pca.mean_, inc_pca.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시간 비교(랜덤 포레스트와 로지스특 회귀분석, 원본 데이터와 압축된 데이터 비교)\n",
    "X_train=mnist['data'][:60000]\n",
    "y_train=mnist['target'][:60000]\n",
    "\n",
    "X_test=mnist['data'][60000:]\n",
    "y_test=mnist['target'][60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 포레스트 모형\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf=RandomForestClassifier(n_estimators=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.24 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10, random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9492"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확도 출력\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred=rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA를 사용하여 분산이 95%가 되도록 차원 축소\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca=PCA(n_components=0.95)\n",
    "X_train_reduced=pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 데이터셋(압축)의 학습 시간 : 9.12s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 랜덤 포레스트 모형에 압축된 데이터 입력\n",
    "rnd_clf2=RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "t0=time.time()\n",
    "rnd_clf2.fit(X_train_reduced, y_train)\n",
    "t1=time.time()\n",
    "print(f\"학습용 데이터셋(압축)의 학습 시간 : {t1-t0:.2f}s\")\n",
    "# 차원 축소가 반드시 학습 시간 단축을 의미하지는 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9009"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증용 데이터셋으로 평가\n",
    "X_test_reduced=pca.transform(X_test)\n",
    "y_pred=rnd_clf2.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)\n",
    "# 차원 축소로 인한 정보 손실로 성능이 감소되는 것이 일반적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 데이터셋(원본)의 학습 시간 : 38.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf=LogisticRegression(multi_class='multinomial', max_iter=300, random_state=42)\n",
    "# multinomial : 소프트맥스 방식의 로지스틱 회귀분석, 시간이 많이 걸림\n",
    "t0=time.time()\n",
    "log_clf.fit(X_train, y_train)\n",
    "t1=time.time()\n",
    "print(f\"학습용 데이터셋(원본)의 학습 시간 : {t1-t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9226"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=log_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "# 랜덤포레스트보다는 성능이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 데이터셋(압축)의 학습 시간 : 39.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 주성분 분석으로 축소된 데이터셋으로 학습\n",
    "log_clf2=LogisticRegression(multi_class=\"multinomial\", max_iter=1000, random_state=42)\n",
    "t0=time.time()\n",
    "log_clf2.fit(X_train_reduced, y_train)\n",
    "t1=time.time()\n",
    "print(f\"학습용 데이터셋(압축)의 학습 시간 : {t1-t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9239"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=log_clf2.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
